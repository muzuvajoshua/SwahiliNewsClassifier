{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/joshua/Desktop/Refactory/SwahiliNewsClassifier/research'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(\"../\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/joshua/Desktop/Refactory/SwahiliNewsClassifier'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from pathlib import Path\n",
    "\n",
    "@dataclass(frozen=True)\n",
    "class TrainingConfig:\n",
    "    \"\"\"\n",
    "    Configuration class for training the model.\n",
    "\n",
    "    Attributes:\n",
    "        root_dir (Path): The root directory where training-related data will be stored or processed.\n",
    "        trained_model_path (Path): The filepath where the trained model will be saved.\n",
    "        training_data (Path): The directory or filepath where training data is located.\n",
    "        params_epochs_1 (int): The number of epochs for fine tuning the LLM\n",
    "        params_epochs_2 (int): The number of epochs for fit 1.\n",
    "        params_epochs_3 (int): The number of epochs for fit 2.\n",
    "        params_epochs_4 (int): The number of epochs for  fit 3.\n",
    "        params_epochs_5 (int): The number of epochs for fit 4.\n",
    "        params_batch_size_1(int): The batch size for the first data loader.\n",
    "        params_batch_size_2 (int): The batch size for the second data loader.\n",
    "        params_learning_rate_1 (float): This is the learning rate for llm\n",
    "        params_learning_rate_2 (float):This is the learning rate for fit 1\n",
    "        params_learning_rate_3 (float):This is the learning rate for fit 2\n",
    "        params_learning_rate_4 (float):This is the learning rate for fit 3\n",
    "        params_learning_rate_5 (float):This is the learning rate for fit 4\n",
    "      \n",
    "    \"\"\"\n",
    "    root_dir: Path\n",
    "    trained_model_path: Path\n",
    "    training_data: Path\n",
    "    params_epochs_1: int\n",
    "    params_epochs_2: int\n",
    "    params_epochs_3: int\n",
    "    params_epochs_4: int\n",
    "    params_epochs_5: int\n",
    "    params_batch_size_1: int\n",
    "    params_batch_size_2: int\n",
    "    params_learning_rate_1:float\n",
    "    params_learning_rate_2:float\n",
    "    params_learning_rate_3:float\n",
    "    params_learning_rate_4:float\n",
    "    params_learning_rate_5:float\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from swahiliNewsClassifier.constants import *\n",
    "from swahiliNewsClassifier.utils.common import read_yaml, create_directories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "class ConfigurationManager:\n",
    "    \"\"\"Class for managing configuration files and preparing base models.\n",
    "    \n",
    "    This class handles the loading of configuration files and parameters,\n",
    "    as well as the creation of directories necessary for preparing base models.\n",
    "    \n",
    "    Attributes:\n",
    "        config_filepath (str, optional): The filepath of the configuration file. Defaults to CONFIG_FILE_PATH.\n",
    "        params_filepath (str, optional): The filepath of the parameters file. Defaults to PARAMS_FILE_PATH.\n",
    "    \n",
    "    Methods:\n",
    "        get_prepare_base_model_config(): Retrieves the configuration for preparing base models.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, config_filepath=CONFIG_FILE_PATH, params_filepath=PARAMS_FILE_PATH):\n",
    "        \"\"\"Initializes the ConfigurationManager.\n",
    "\n",
    "        Args:\n",
    "            config_filepath (str, optional): The filepath of the configuration file. Defaults to CONFIG_FILE_PATH.\n",
    "            params_filepath (str, optional): The filepath of the parameters file. Defaults to PARAMS_FILE_PATH.\n",
    "        \"\"\"\n",
    "        self.config = read_yaml(config_filepath)\n",
    "        self.params = read_yaml(params_filepath)\n",
    "\n",
    "        create_directories([self.config.artifacts_root])\n",
    "\n",
    "    def get_training_config(self) -> TrainingConfig:\n",
    "        \"\"\"\n",
    "        Retrieves the training configuration parameters and constructs a TrainingConfig object.\n",
    "\n",
    "        This method extracts the training configuration parameters from the overall configuration and parameters files,\n",
    "        constructs the path to the training data directory, creates necessary directories, and packages all the parameters\n",
    "        into a TrainingConfig object.\n",
    "\n",
    "        Returns:\n",
    "            TrainingConfig: An instance of TrainingConfig containing the training configuration parameters.\n",
    "\n",
    "        Raises:\n",
    "            ValueError: If any required configuration parameter is missing or invalid.\n",
    "        \"\"\"\n",
    "        training = self.config.training\n",
    "        params = self.params\n",
    "\n",
    "        training_data = os.path.join(self.config.data_ingestion.unzip_dir, \"dataset/train\")\n",
    "\n",
    "        create_directories([Path(training.root_dir)])\n",
    "\n",
    "        training_config = TrainingConfig(\n",
    "            root_dir=Path(training.root_dir),\n",
    "            trained_model_path=Path(training.trained_model_path),\n",
    "            training_data=Path(training_data),\n",
    "            params_epochs_1=params.EPOCHS_1,\n",
    "            params_epochs_2=params.EPOCHS_2,\n",
    "            params_epochs_3=params.EPOCHS_3,\n",
    "            params_epochs_4=params.EPOCHS_4,\n",
    "            params_epochs_5=params.EPOCHS_5,\n",
    "            params_batch_size_1=params.BATCH_SIZE_1,\n",
    "            params_batch_size_2=params.BATCH_SIZE_2,\n",
    "            params_learning_rate_1 = params.LEARNING_RATE_1,\n",
    "            params_learning_rate_2 = params.LEARNING_RATE_2,\n",
    "            params_learning_rate_3 = params.LEARNING_RATE_3,\n",
    "            params_learning_rate_4 = params.LEARNING_RATE_4,\n",
    "            params_learning_rate_5 = params.LEARNING_RATE_5           \n",
    "\n",
    "           \n",
    "        )\n",
    "\n",
    "        return training_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'torch._C'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mtorch\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mfastai\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mfastai\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mtext\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mall\u001b[39;00m \u001b[39mimport\u001b[39;00m \u001b[39m*\u001b[39m\n\u001b[1;32m      4\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mpandas\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mpd\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mnumpy\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mnp\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/swahili2/lib/python3.10/site-packages/fastai/text/all.py:1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mbasics\u001b[39;00m \u001b[39mimport\u001b[39;00m \u001b[39m*\u001b[39m\n\u001b[1;32m      2\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mcallback\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mall\u001b[39;00m \u001b[39mimport\u001b[39;00m \u001b[39m*\u001b[39m\n\u001b[1;32m      3\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mcore\u001b[39;00m \u001b[39mimport\u001b[39;00m \u001b[39m*\u001b[39m\n",
      "File \u001b[0;32m~/anaconda3/envs/swahili2/lib/python3.10/site-packages/fastai/basics.py:1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mdata\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mall\u001b[39;00m \u001b[39mimport\u001b[39;00m \u001b[39m*\u001b[39m\n\u001b[1;32m      2\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39moptimizer\u001b[39;00m \u001b[39mimport\u001b[39;00m \u001b[39m*\u001b[39m\n\u001b[1;32m      3\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mcallback\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mcore\u001b[39;00m \u001b[39mimport\u001b[39;00m \u001b[39m*\u001b[39m\n",
      "File \u001b[0;32m~/anaconda3/envs/swahili2/lib/python3.10/site-packages/fastai/data/all.py:1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mtorch_basics\u001b[39;00m \u001b[39mimport\u001b[39;00m \u001b[39m*\u001b[39m\n\u001b[1;32m      2\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mcore\u001b[39;00m \u001b[39mimport\u001b[39;00m \u001b[39m*\u001b[39m\n\u001b[1;32m      3\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mload\u001b[39;00m \u001b[39mimport\u001b[39;00m \u001b[39m*\u001b[39m\n",
      "File \u001b[0;32m~/anaconda3/envs/swahili2/lib/python3.10/site-packages/fastai/torch_basics.py:1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtorch\u001b[39;00m \u001b[39mimport\u001b[39;00m multiprocessing\n\u001b[1;32m      2\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mplatform\u001b[39;00m\u001b[39m,\u001b[39m\u001b[39mos\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[39mif\u001b[39;00m platform\u001b[39m.\u001b[39msystem()\u001b[39m==\u001b[39m\u001b[39m'\u001b[39m\u001b[39mDarwin\u001b[39m\u001b[39m'\u001b[39m:\n\u001b[1;32m      4\u001b[0m     \u001b[39m# Python 3.8 changed to 'spawn' but that doesn't work with PyTorch DataLoader w n_workers>0\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/swahili2/lib/python3.10/site-packages/torch/multiprocessing/__init__.py:20\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39msys\u001b[39;00m\n\u001b[1;32m     19\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mtorch\u001b[39;00m\n\u001b[0;32m---> 20\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mreductions\u001b[39;00m \u001b[39mimport\u001b[39;00m init_reductions\n\u001b[1;32m     22\u001b[0m __all__ \u001b[39m=\u001b[39m [\u001b[39m\"\u001b[39m\u001b[39mset_sharing_strategy\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mget_sharing_strategy\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mget_all_sharing_strategies\u001b[39m\u001b[39m\"\u001b[39m]\n\u001b[1;32m     25\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mmultiprocessing\u001b[39;00m \u001b[39mimport\u001b[39;00m \u001b[39m*\u001b[39m  \u001b[39m# noqa: F403\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/swahili2/lib/python3.10/site-packages/torch/multiprocessing/reductions.py:9\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtyping\u001b[39;00m \u001b[39mimport\u001b[39;00m Union\n\u001b[1;32m      8\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mtorch\u001b[39;00m\n\u001b[0;32m----> 9\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mtorch\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mutils\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mhooks\u001b[39;00m\n\u001b[1;32m     10\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtorch\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39m_namedtensor_internals\u001b[39;00m \u001b[39mimport\u001b[39;00m check_serializing_named_tensor\n\u001b[1;32m     12\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m     13\u001b[0m     \u001b[39m# Early load resource_sharer to prevent a partially initialized instance\u001b[39;00m\n\u001b[1;32m     14\u001b[0m     \u001b[39m# from being inherited in a forked child process. The reduce_storage method\u001b[39;00m\n\u001b[1;32m     15\u001b[0m     \u001b[39m# requires this module indirectly through DupFd(). The built-in mp.Queue\u001b[39;00m\n\u001b[1;32m     16\u001b[0m     \u001b[39m# class pickles arguments in a background thread which may overlap with the\u001b[39;00m\n\u001b[1;32m     17\u001b[0m     \u001b[39m# fork.\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/swahili2/lib/python3.10/site-packages/torch/utils/__init__.py:4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mos\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpath\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39m_osp\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mtorch\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mthroughput_benchmark\u001b[39;00m \u001b[39mimport\u001b[39;00m ThroughputBenchmark\n\u001b[1;32m      5\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mcpp_backtrace\u001b[39;00m \u001b[39mimport\u001b[39;00m get_cpp_backtrace\n\u001b[1;32m      6\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mbackend_registration\u001b[39;00m \u001b[39mimport\u001b[39;00m rename_privateuse1_backend, generate_methods_for_privateuse1_backend\n",
      "File \u001b[0;32m~/anaconda3/envs/swahili2/lib/python3.10/site-packages/torch/utils/throughput_benchmark.py:2\u001b[0m\n\u001b[0;32m----> 2\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mtorch\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39m_C\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mformat_time\u001b[39m(time_us\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, time_ms\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, time_s\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n\u001b[1;32m      6\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Define time formatting.\"\"\"\u001b[39;00m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'torch._C'"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import fastai\n",
    "from fastai.text.all import *\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from functools import partial\n",
    "import io\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SwahiliNewsClassifier:\n",
    "    def __init__(self, training_config:TrainingConfig):\n",
    "        self.config = training_config\n",
    "        self.df_trn = None\n",
    "        self.df_val = None\n",
    "        self.df_lm = None\n",
    "        self.dls_lm = None\n",
    "        self.learn_lm = None\n",
    "        self.dls_classifier = None\n",
    "        self.learn_classifier = None\n",
    "\n",
    "    def preprocess_data(self):\n",
    "        self.df_trn, self.df_val = train_test_split(self.config.training_data, stratify=self.config.training_data['category'], test_size=0.3, random_state=123)\n",
    "        self.df_lm = pd.concat([self.df_trn, self.df_val], axis=0)[['content']]\n",
    "\n",
    "    def prepare_language_model(self, lm_epochs=self.config.params_epochs_1, learning_rate =self.config.params_learning_rate_1, bs = self.config.params_batch_1):\n",
    "        \n",
    "        # Language Model\n",
    "        dblock = DataBlock(\n",
    "            blocks=TextBlock.from_df('content', is_lm=True),\n",
    "            get_x=ColReader('text'),\n",
    "            splitter=RandomSplitter(0.1))\n",
    "        self.dls_lm = dblock.dataloaders(self.df_lm, bs)\n",
    "        self.learn_lm = language_model_learner(self.dls_lm, AWD_LSTM, drop_mult=0.3, metrics=[accuracy]).to_fp16()\n",
    "        self.learn_lm.lr_find()\n",
    "        self.learn_lm.fine_tune(lm_epochs, learning_rate)\n",
    "        self.learn_lm.save_encoder('finetuned')\n",
    "\n",
    "    def prepare_classifier_learner(self,bs=self.config.params_batch_2, layer_1_epochs = self.config.params_epochs_2,\n",
    "                            layer_2_epochs = self.config.params_epochs_3,layer_3_epochs = self.config.params_epochs_4, \n",
    "                            layer_4_epochs = self.config.params_epochs_5, layer_1_lr =self.config.params_learning_rate_2,\n",
    "                            layer_2_lr =self.config.params_learning_rate_3,layer_3_lr =self.config.params_learning_rate_4,\n",
    "                            layer_4_lr =self.config.params_learning_rate_5,):\n",
    "        # Classifier Learner\n",
    "        blocks = (TextBlock.from_df('content', seq_len=self.dls_lm.seq_len, vocab=self.dls_lm.vocab), CategoryBlock())\n",
    "        dls = DataBlock(\n",
    "            blocks=blocks,\n",
    "            get_x=ColReader('text'),\n",
    "            get_y=ColReader('category'),\n",
    "            splitter=RandomSplitter(0.2))\n",
    "        self.dls_classifier = dls.dataloaders(self.df_trn, bs)\n",
    "        self.learn_classifier = text_classifier_learner(self.dls_classifier, AWD_LSTM, metrics=[accuracy]).to_fp16()\n",
    "        encoder_path_1 = os.path.join(self.config.trained_model_path, 'finetuned_llm')\n",
    "        self.learn_classifier.load_encoder(encoder_path_1)\n",
    "        self.learn_classifier.lr_find()\n",
    "        self.learn_classifier.fit_one_cycle(layer_1_epochs, layer_1_lr)\n",
    "        self.learn_classifier.freeze_to(-2)\n",
    "        self.learn_classifier.fit_one_cycle(layer_2_epochs, slice(1e-3/(2.6**4),layer_2_lr))\n",
    "        self.learn_classifier.freeze_to(-3)\n",
    "        self.learn_classifier.fit_one_cycle(layer_3_epochs, slice(5e-3/(2.6**4),layer_3_lr))\n",
    "        self.learn_classifier.unfreeze()\n",
    "        self.learn_classifier.fit_one_cycle(layer_4_epochs, slice(1e-3/(2.6**4),layer_4_lr))\n",
    "        encoder_path_2 = os.path.join(self.config.trained_model_path, 'SwahiliNewsclassifier')\n",
    "        self.learn_classifier.save_encoder(encoder_path_2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    config = ConfigurationManager()\n",
    "    training_config = config.get_training_config()\n",
    "    training = SwahiliNewsClassifier(config=training_config)\n",
    "    training.preprocess_data()\n",
    "    training.prepare_language_model()\n",
    "    training.prepare_classifier_learner()\n",
    "except Exception as e:\n",
    "    raise e"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "swahili2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
